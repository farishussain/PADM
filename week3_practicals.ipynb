{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week3 - Practicals - PADM\n",
    "----------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda for the day\n",
    "1. Install the Gym Library\n",
    "\n",
    "    1. Classic Control (eg. Cartpole) --> https://gymnasium.farama.org/environments/classic_control/\n",
    "    2. Toy Text (eg. Frozen Lake) --> https://gymnasium.farama.org/environments/toy_text/\n",
    "    3. Atari (eg. Breakout) --> https://gymnasium.farama.org/environments/atari/breakout/\n",
    "2. Demonstration of Gym Environments.\n",
    "3. Lets Build one Gym env and understand what action,state,step and step are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic control\n",
    "\n",
    "#### pip install gymnasium[classic_control]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "for _ in range(100):\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    pole_angle = observation[2]\n",
    "    action = 0 if pole_angle < 0 else 1\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Text\n",
    "#### pip install gymnasium[toy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Atari\n",
    "#### pip install \"gymnasium[atari, accept-rom-license]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Breakout, Pong\n",
    "\n",
    "env = gym.make('ALE/Breakout-v5', render_mode='human')\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll explore the basic concepts using the OpenAI Gym's CartPole-v1 environment. The key concepts we'll cover include:\n",
    "\n",
    "* *State*: The current situation of the environment which the agent observes.\n",
    "* *Action*: The decision or move made by the agent based on the state.\n",
    "* *Reward*: Feedback from the environment based on the action taken.\n",
    "* *Step*: A function that moves the environment to the next state based on the action and returns the new state, reward, termination condition, and additional info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Space\n",
    "\n",
    "* 0: Push cart to the left\n",
    "\n",
    "* 1: Push cart to the right\n",
    "\n",
    "#### Observation Space\n",
    "\n",
    "| Num | Observation | Min | Max |\n",
    "|---|---|---|---|\n",
    "| 0 | Cart Position | -4.8 | 4.8 |\n",
    "| 1 | Cart Velocity | -Inf | Inf |\n",
    "| 2 | Pole Angle | -0.418 rad (-24°) | 0.418 rad (24°) |\n",
    "| 3 | Pole Angular Velocity | -Inf | Inf |\n",
    "\n",
    "#### Rewards\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 500 for v1 and 200 for v0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### env.step(action) --> https://gymnasium.farama.org/api/env/\n",
    "\n",
    "RETURNS:\n",
    "\n",
    "* observation (ObsType) – An element of the environment’s observation_space as the next observation due to the agent actions. An example is a numpy array containing the positions and velocities of the pole in CartPole.\n",
    "\n",
    "* reward (SupportsFloat) – The reward as a result of taking the action.\n",
    "\n",
    "* terminated (bool) – Whether the agent reaches the terminal state (as defined under the MDP of the task) which can be positive or negative. An example is reaching the goal state or moving into the lava from the Sutton and Barton, Gridworld. If true, the user needs to call reset().\n",
    "\n",
    "* truncated (bool) – Whether the truncation condition outside the scope of the MDP is satisfied. Typically, this is a timelimit, but could also be used to indicate an agent physically going out of bounds. Can be used to end the episode prematurely before a terminal state is reached. If true, the user needs to call reset().\n",
    "\n",
    "* info (dict) – Contains auxiliary diagnostic information (helpful for debugging, learning, and logging). This might, for instance, contain: metrics that describe the agent’s performance state, variables that are hidden from observations, or individual reward terms that are combined to produce the total reward. In OpenAI Gym <v26, it contains “TimeLimit.truncated” to distinguish truncation and termination, however this is deprecated in favour of returning terminated and truncated variables.\n",
    "\n",
    "* done (bool) – (Deprecated) A boolean value for if the episode has ended, in which case further step() calls will return undefined results. This was removed in OpenAI Gym v26 in favor of terminated and truncated attributes. A done signal may be emitted for different reasons: Maybe the task underlying the environment was solved successfully, a certain timelimit was exceeded, or the physics simulation has entered an invalid state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building and Understanding the CartPole Environment\n",
    "In this exercise, you will get hands-on experience with one of the most iconic environments in reinforcement learning—the CartPole. Your task will be to build the environment, interact with it using a simple policy, and observe the effects of actions on the state and overall performance.\n",
    "\n",
    "### Part 1: Setup and Environment Creation\n",
    "##### Task 1.1: First, you need to import the necessary library. Use the import statement to import gym.\n",
    "##### Task 1.2: Now, create an instance of the CartPole-v1 environment and assign it to a variable named env. Remember to set render_mode=\"human\" to visualize the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO: Import the gym library\n",
    "* TODO: Create the CartPole-v1 environment with render_mode set to \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder to add your imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Click to see the solution for Part 1</summary>\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Interacting with the Environment\n",
    "Before diving into policy implementation, it's important to understand how to interact with the Gym environment.\n",
    "\n",
    "##### Task 2.1: Reset the environment to its initial state using the reset method and print the initial observation.\n",
    "\n",
    "* TODO: Reset the environment and print the initial observation\n",
    "\n",
    "##### Task 2.2: Implement a loop to simulate the agent taking 100 random actions in the environment. At each step, use the action_space.sample() method to choose a random action, apply it to the environment using the step function, and print the new observation and reward.\n",
    "\n",
    "* TODO: Simulate 100 steps in the environment using random actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the solution for Part 2</summary>\n",
    "\n",
    "```python\n",
    "observation, info = env.reset()\n",
    "print(\"Initial Observation:\", observation)\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Take a random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"New Observation: {observation}, Reward: {reward}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Implementing a Simple Policy\n",
    "Now that you're familiar with the basics of interacting with the environment, let's implement a simple policy to control the cart.\n",
    "\n",
    "##### Task 3.1: Create a policy function that takes an observation as input and returns an action. For this simple policy, return action 0 (move left) if the pole's angle (second value of the observation array) is negative, and action 1 (move right) if the angle is positive.\n",
    "\n",
    "* TODO: Apply the simple policy in the simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Placeholder for implementation\n",
    "def simple_policy():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the solution for Part 3</summary>\n",
    "\n",
    "```python\n",
    "def simple_policy(observation):\n",
    "    return 0 if observation[2] < 0 else 1\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(100):\n",
    "    action = simple_policy(observation)  # Use the simple policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Custom Environment\n",
    "\n",
    "* https://github.com/Farama-Foundation/Gymnasium/tree/main\n",
    "* https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "padm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
